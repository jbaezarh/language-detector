{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9692973e",
   "metadata": {},
   "source": [
    "# Modelo basado en n-gramas\n",
    "\n",
    "Comenzaremos con una implementación basada en *Cavnar, W. B., \\& Trenkle, J. M. (1994). N-Gram-Based Text Categorization.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d14984",
   "metadata": {},
   "source": [
    "**Idea:**\n",
    "- **Entrenamiento**: Entrenar el método es construir los perfiles de cada uno de los idiomas en base a nuestro corpus. Un perfil de un idioma es la frecuencia de los n-gramas que más veces han aparecido en nuestro corpus (en el paper original se quedan con los 300 primeros). Se usan los n-gramas de longitud 1 hasta n (en el paper original se toma n=5).\n",
    "\n",
    "- **Predicción**: Para predecir un nuevo texto primero se obtiene el perfil de n-gramas de este. Después se compara con los que conocemos, empleando la distancia \"out-of-place\", básicamente comparando el orden de los n-gramas del perfil. Y se le asigna el idioma con la menor distancia.\n",
    "\n",
    "- **Preprocesamiento**:\n",
    "    > Split the text into separate tokens consist\n",
    "    > ing only of letters and apostrophes. Digits\n",
    "    > and punctuation are discarded. Pad the\n",
    "    > token with sufficient blanks before and after.\n",
    "\n",
    "    - Pasar todo a minúscula\n",
    "    - No se eliminan los espacios\n",
    "    - Eliminar los números y signos de puntuación (manteniendo los apóstrofes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64fda30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías necesarias\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "from typing import List, Dict, Union\n",
    "import joblib\n",
    "import pickle\n",
    "from utils import classification_metrics_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d0ef11",
   "metadata": {},
   "source": [
    "## 1. Definición del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e87837a",
   "metadata": {},
   "source": [
    "Definimos la clase para el modelo de N-gramas con los métodos `fit` y `predict`. He optado por incluir el preprocesamiento como un método (`_preprocess_texts`) dentro de la clase al que se llama de forma automática al entrenar el modelo y hacer predicciones. De esta forma, evitamos inconsistencias y queda todo mejor encapsulado, ya que el preprocesamiento de este modelo es particular en coparación con el que emplearemos para los otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee48678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageIdentifier:\n",
    "    \"\"\"\n",
    "    Clase con una implementación de un modelo modelo de detección automática de \n",
    "    idiomas basado en N-gramas inspirado en\n",
    "    Cavnar, W. B., & Trenkle, J. M. (1994). N-Gram-Based Text Categorization\n",
    "\n",
    "    Args:\n",
    "    - n: Tamaño máximo de los n-gramas a emplear. Se emplean los n-gramas de \n",
    "         tamaño 1 hasta n (por defecto 5)\n",
    "    - top_k: Tamaño del perfil de n-gramas construido (por defecto 300)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n: int = 5, top_k: int = 300):\n",
    "        self.n = n\n",
    "        self.top_k = top_k\n",
    "        self.language_profiles: Dict[str, Dict[str, int]] = {}\n",
    "\n",
    "    def _get_ngrams(self, text: str, n: int) -> List[str]:\n",
    "        \"\"\" \n",
    "        Obtener todos los n-gramas de longitud n de text. \n",
    "        Se aplica padding de acuerdo al artículo.\n",
    "        \"\"\"\n",
    "        # Se transforman todos los carácteres de espacio `[ \\t\\n\\r\\f\\v]` a un espacio\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Se eliminan los espacios al principio y al final\n",
    "        text = text.strip()\n",
    "        # Se añade padding al principio y al final (estandarizar)\n",
    "        text = ' ' * (n - 1) + text + ' ' * (n - 1)\n",
    "        # Se devuelven los n-gramas \n",
    "        return [text[i:i+n] for i in range(len(text) - n + 1)]\n",
    "\n",
    "    def _build_profile(self, texts: List[str]) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Construir el perfil de n-gramas en base a los textos de texts.\n",
    "        Se consideran todos los n-gramas de longitud 1 hasta self.n\n",
    "        \"\"\"\n",
    "        ngram_counter = Counter()\n",
    "        for i in range(1, self.n + 1):\n",
    "            for text in texts:\n",
    "                ngram_counter.update(self._get_ngrams(text.lower(), i))\n",
    "        most_common = ngram_counter.most_common(self.top_k)\n",
    "        return {ngram: rank for rank, (ngram, _) in enumerate(most_common)}\n",
    "\n",
    "    def _rank_order_distance(self, profile1: Dict[str, int], profile2: Dict[str, int]) -> int:\n",
    "        \"\"\"\"\n",
    "        Devuelve la distancia \"out of place\" entre los perfiles profile1 y profile2.\n",
    "        \"\"\"\n",
    "        # El término de penalización creo que es la diferencia máxima\n",
    "        penalty = len(profile2) + 1\n",
    "\n",
    "        distance = 0\n",
    "        for ngram in profile1:\n",
    "            if ngram in profile2:\n",
    "                # Si el n-grama está en el profile2, se suma la diferencia en la posición\n",
    "                distance += abs(profile1[ngram] - profile2[ngram])\n",
    "            else:\n",
    "                # Si no se le suma un término de penalización\n",
    "                distance += penalty\n",
    "        return distance\n",
    "    \n",
    "    def _preprocess_texts(self,texts: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Preprocesamiento necesario para el modelo\n",
    "        \"\"\"\n",
    "        cleaned_texts = []\n",
    "        for text in texts:\n",
    "            # Convertir a minúsculas\n",
    "            text = text.lower()\n",
    "            # Eliminar todo lo que no sea letra unicode, espacio o apóstrofo (dejando un espacio)\n",
    "            text = re.sub(r\"[^\\p{L}\\s']\", ' ', text)\n",
    "            # Eliminar guiones bajos (porque \\w incluye _)\n",
    "            text = re.sub(r'_', '', text)\n",
    "            # Convertir espacios múltiples en uno solo\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "            cleaned_texts.append(text)\n",
    "        return cleaned_texts\n",
    "\n",
    "    def fit(self, train_texts:List[str], y_train:List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Construir los perfiles de n-gramas para cada idioma\n",
    "        \"\"\"\n",
    "        # Preprocesamos los datos\n",
    "        train_texts = self._preprocess_texts(train_texts)\n",
    "\n",
    "        # Agrupar los datos por idioma\n",
    "        training_data = defaultdict(list)\n",
    "        for text, label in zip(train_texts, y_train):\n",
    "             training_data[label].append(text)\n",
    "\n",
    "        # Construir los perfiles para cada idioma\n",
    "        self.language_profiles = {\n",
    "            lang: self._build_profile(texts)\n",
    "            for lang, texts in training_data.items()\n",
    "        }\n",
    "\n",
    "    def predict(self, test_texts:Union[str, List[str]], distances:bool=False) -> Union[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Predecir un nuevo texto o lista de textos. Si es un solo texto\n",
    "        también se pueden devolver las distancias (inversas del score)\n",
    "        \"\"\"\n",
    "\n",
    "        # Si es una sola cadena, convertirla a lista temporalmente\n",
    "        is_single = isinstance(test_texts, str)\n",
    "        if is_single:\n",
    "            test_texts = [test_texts]\n",
    "\n",
    "        # Preprocesamos los datos\n",
    "        test_texts = self._preprocess_texts(test_texts)\n",
    "\n",
    "        predictions = []    \n",
    "        for text in test_texts:\n",
    "            # Se construye el perfil del nuevo texto\n",
    "            test_profile = self._build_profile([text])\n",
    "            # Se mide la distancia a los perfiles aprendidos\n",
    "            distances = {\n",
    "                lang: self._rank_order_distance(test_profile, profile)\n",
    "                for lang, profile in self.language_profiles.items()\n",
    "            }\n",
    "            # Se devuelve el mínimo de la distancia\n",
    "            predicted = min(distances, key=distances.get)\n",
    "            predictions.append(predicted)\n",
    "\n",
    "        if is_single:\n",
    "            return distances if distances else predictions[0]\n",
    "        \n",
    "        else:\n",
    "            return  predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ebc247",
   "metadata": {},
   "source": [
    "## 2. Aplicación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db2acaf",
   "metadata": {},
   "source": [
    "### Lectura de los datos\n",
    "Comenzamos leyendo las particiones de entrenamiento y test que habíamos creado previamente. En este caso no vamos a hacer ajuste de hiperparámetros por lo que no vamos a emplear el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "445d83b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text language  n_words\n",
      "0   Jeśli chcecie dziś się zbłaźnić, to zapraszam.\\n       pl        7\n",
      "1  Los hombres estaban en el frente por aquel ent...       es        9\n",
      "2  Ορθώς, επειδή η αρχική πρόταση απέβλεπε μόνο σ...       el       10\n",
      "3  De modo que en ese sentido hay un claro equili...       es       10\n",
      "4  V tem kontekstu moramo izpostaviti kmetijski s...       sl        7\n"
     ]
    }
   ],
   "source": [
    "# Leemos los datos\n",
    "df_train = pd.read_csv(os.path.join(\"data\",\"raw\",\"train.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(\"data\",\"raw\",\"test.csv\"))\n",
    "\n",
    "# Veamos nuestro DataFrame\n",
    "print(df_train.head(5))\n",
    "\n",
    "# Separamos las variables que usaremos\n",
    "train_texts = df_train[\"text\"]\n",
    "test_texts = df_test[\"text\"]\n",
    "\n",
    "y_train = df_train[\"language\"]\n",
    "y_test = df_test[\"language\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3e49fa",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo\n",
    "Definimos y entrenamos el modelo. Dado que hemos incluido el preprocesamiento dentro del modelo, no tenemos que preocuparnos por hacerlo previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32c03475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el modelo\n",
    "ngrams = NGramLanguageIdentifier(n=5, top_k=1000)\n",
    "\n",
    "# Lo entrenamos\n",
    "ngrams.fit(train_texts, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdad3296",
   "metadata": {},
   "source": [
    "Vamos a guardar el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9045ade6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models\\\\n-grams\\\\ngrams_model.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos el directorio si no existe\n",
    "ngram_dir = os.path.join(\"models\",\"n-grams\")\n",
    "os.makedirs(ngram_dir, exist_ok=True)\n",
    "\n",
    "# Guardamos el modelo entrenado\n",
    "joblib.dump(ngrams, os.path.join(ngram_dir,'ngrams_model.joblib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d8d87f",
   "metadata": {},
   "source": [
    "Veamos un ejemplo de cómo cargarlo posteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35f507da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cargar el modelo guardado\n",
    "# loaded_model = joblib.load(os.path.join(ngram_dir,'ngrams_model.joblib'))\n",
    "\n",
    "# # Usar el modelo cargado\n",
    "# loaded_model.predict(\"Bonjour à tous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3100315c",
   "metadata": {},
   "source": [
    "### Evaluación del modelo\n",
    "Empleamos el modelo para hacer predicciones sobre el conjunto de test y evaluamos su rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4f72d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Global Metrics ===\n",
      " accuracy  precision_macro  recall_macro  f1_macro\n",
      " 0.990557         0.990554      0.990527   0.99053\n",
      "\n",
      "=== Per-Class Metrics ===\n",
      "label  precision   recall       f1  support\n",
      "   cs   1.000000 0.987535 0.993728      722\n",
      "   da   0.981358 0.987936 0.984636      746\n",
      "   de   0.997372 0.996063 0.996717      762\n",
      "   el   1.000000 0.994543 0.997264      733\n",
      "   es   0.976712 0.980743 0.978723      727\n",
      "   fi   0.998728 1.000000 0.999363      785\n",
      "   fr   0.992136 0.997365 0.994744      759\n",
      "   it   0.989276 0.995951 0.992603      741\n",
      "   pl   0.994543 0.990489 0.992512      736\n",
      "   pt   0.982667 0.977454 0.980053      754\n",
      "   sl   0.983979 0.994602 0.989262      741\n",
      "   sv   0.989873 0.983648 0.986751      795\n",
      "\n",
      "=== Metrics Per Size ===\n",
      "  size  accuracy  precision_macro  recall_macro  f1_macro\n",
      " large  0.999635         0.999653      0.999638  0.999644\n",
      "medium  0.996881         0.996909      0.996900  0.996901\n",
      " small  0.960586         0.959667      0.960054  0.959672\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "     cs   da   de   el   es   fi   fr   it   pl   pt   sl   sv\n",
      "cs  713    0    0    0    0    0    0    1    0    0    8    0\n",
      "da    0  737    1    0    0    0    1    1    0    0    0    6\n",
      "de    0    0  759    0    0    0    1    0    0    0    0    2\n",
      "el    0    0    0  729    0    0    1    0    2    1    0    0\n",
      "es    0    0    0    0  713    0    1    2    0   11    0    0\n",
      "fi    0    0    0    0    0  785    0    0    0    0    0    0\n",
      "fr    0    1    0    0    0    0  757    1    0    0    0    0\n",
      "it    0    1    0    0    1    0    0  738    0    1    0    0\n",
      "pl    0    0    1    0    1    0    1    0  729    0    4    0\n",
      "pt    0    0    0    0   14    0    0    2    1  737    0    0\n",
      "sl    0    2    0    0    0    1    1    0    0    0  737    0\n",
      "sv    0   10    0    0    1    0    0    1    1    0    0  782\n"
     ]
    }
   ],
   "source": [
    "# Hacemos las predicciones\n",
    "preds = ngrams.predict(test_texts)\n",
    "\n",
    "# Empleamos una función personalizada para ver un report del rendimiento\n",
    "results_ngram = classification_metrics_report(y_test, preds, df_test[\"n_words\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47b525a",
   "metadata": {},
   "source": [
    "- El modelo funciona muy bien, lo cual hay que sumar a lo simple y rápido que es. \n",
    "- Al aumentar el tamaño del perfil de ngramas mejora significativamente el rendimiento del modelo.\n",
    "- Donde más fallos se producen es entre el español y el portugués y entre el sueco y el danés, ambos casos en los que habíamos visto que hay muchas palabras comunes. \n",
    "- A medida que aumenta el tamaño de las frases, aumenta el rendimiento del modelo como es de esperar, pero con frases de tamaño medio (7 a 12 palabras), el modelo tiene un rendimiento excelente.\n",
    "- Tanto el checo como el griego tienen una precisión de 1, es decir, en estos idiomas no hay falsos positivos. Cuando el modelo dice que una frase está en uno de estos idiomas no se equivoca. Esto es de esperar, ya que son idiomas muy caracteríscos, en especial el griego que tiene un alfabeto diferente.\n",
    "- El finés tiene un recall de 1, esto significa que no hay falsos negativos. Cuando aparece algo en finés, el modelo lo detecta perfectamente. Esto tampoco nos sorprende porque es un idioma bastante característico e incluso tiene letras propias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6045034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos los resultados\n",
    "with open(os.path.join(ngram_dir,\"results.json\"), \"wb\") as f:\n",
    "    pickle.dump(results_ngram, f)\n",
    "\n",
    "# Para cargarlo\n",
    "# with open(os.path.join(ngram_dir,\"results.json\"), \"rb\") as f:\n",
    "#     results_ngram = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a47fe",
   "metadata": {},
   "source": [
    "### Análisis de errores\n",
    "Veamos donde se equivoca el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b488cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función nos transformará las distancias en scores, lo cual será útil de cara\n",
    "# a entender los errores del modelo\n",
    "\n",
    "def distances2scores(distances: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Convierte distancias en scores usando el inverso (1/d),\n",
    "    y normaliza para que sumen 1 (tipo distribución de probabilidad).\n",
    "\n",
    "    Args:\n",
    "        distances (dict): Diccionario de distancias (valores > 0).\n",
    "\n",
    "    Returns:\n",
    "        dict: Scores normalizados (más cercanos → mayor score).\n",
    "    \"\"\"\n",
    "    # Evitar división por cero\n",
    "    eps = 1e-8\n",
    "    inv = {k: 1 / (v + eps) for k, v in distances.items()}\n",
    "    total = sum(inv.values())\n",
    "    \n",
    "    scores = {k: round(v / total, 3) for k, v in inv.items()}\n",
    "\n",
    "    # Lo devolvemos ordenado en orden decreciente\n",
    "    return dict(sorted(scores.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af5243c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "language",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "n_words",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "pred",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "scores",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "preprocessed_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "2dd82190-491d-40e4-9c5d-34679fc88537",
       "rows": [
        [
         "166",
         "E' importante dirlo.\n",
         "it",
         "3",
         "es",
         "{'es': 0.102, 'it': 0.1, 'pt': 0.099, 'fr': 0.087, 'sl': 0.083, 'de': 0.083, 'sv': 0.079, 'da': 0.079, 'fi': 0.077, 'pl': 0.076, 'cs': 0.076, 'el': 0.059}",
         "e' importante dirlo"
        ],
        [
         "394",
         "(La sesión, suspendida a las 13.10 horas, se reanuda a las 15.00 horas)\n",
         "pl",
         "11",
         "es",
         "{'es': 0.11, 'pt': 0.091, 'fr': 0.088, 'it': 0.088, 'sl': 0.081, 'cs': 0.081, 'fi': 0.081, 'de': 0.08, 'sv': 0.08, 'da': 0.079, 'pl': 0.077, 'el': 0.063}",
         "la sesión suspendida a las horas se reanuda a las horas"
        ],
        [
         "681",
         "Han fyllde visst år i går.\n",
         "sv",
         "6",
         "da",
         "{'da': 0.101, 'sv': 0.099, 'de': 0.086, 'fr': 0.085, 'es': 0.082, 'it': 0.082, 'fi': 0.082, 'pt': 0.081, 'sl': 0.08, 'cs': 0.08, 'pl': 0.077, 'el': 0.064}",
         "han fyllde visst år i går"
        ],
        [
         "740",
         "Primero vamos a votar el corrigendum nº 1, para ver si se acepta o no.\n",
         "es",
         "14",
         "pt",
         "{'pt': 0.097, 'es': 0.096, 'it': 0.09, 'sl': 0.087, 'fr': 0.084, 'cs': 0.083, 'sv': 0.082, 'da': 0.082, 'de': 0.081, 'pl': 0.078, 'fi': 0.078, 'el': 0.063}",
         "primero vamos a votar el corrigendum nº para ver si se acepta o no"
        ],
        [
         "903",
         "- Έκθεση Costa\n",
         "el",
         "2",
         "pt",
         "{'pt': 0.09, 'it': 0.09, 'es': 0.088, 'fi': 0.087, 'pl': 0.084, 'sl': 0.084, 'el': 0.083, 'cs': 0.082, 'sv': 0.08, 'fr': 0.08, 'da': 0.076, 'de': 0.075}",
         "έκθεση costa"
        ],
        [
         "1113",
         "Lembro­me perfeitamente.\n",
         "pt",
         "3",
         "it",
         "{'it': 0.103, 'es': 0.1, 'pt': 0.1, 'fr': 0.09, 'sl': 0.08, 'sv': 0.08, 'da': 0.08, 'de': 0.079, 'cs': 0.078, 'fi': 0.077, 'pl': 0.076, 'el': 0.056}",
         "lembro me perfeitamente"
        ],
        [
         "1181",
         "(10419/1/2001 - C5-0417/2001 - 2000/0188(COD)) (Εισηγήτρια: κ. Niebler)\n",
         "el",
         "4",
         "fr",
         "{'el': 0.086, 'fr': 0.086, 'da': 0.086, 'es': 0.085, 'pl': 0.084, 'sv': 0.084, 'pt': 0.083, 'it': 0.083, 'sl': 0.082, 'de': 0.082, 'cs': 0.081, 'fi': 0.078}",
         "c cod εισηγήτρια κ niebler"
        ],
        [
         "1229",
         "Komise za sebe tvrdí opak.\n",
         "cs",
         "5",
         "sl",
         "{'sl': 0.098, 'cs': 0.096, 'pl': 0.09, 'fi': 0.087, 'sv': 0.083, 'da': 0.083, 'es': 0.082, 'de': 0.081, 'pt': 0.081, 'it': 0.079, 'fr': 0.077, 'el': 0.062}",
         "komise za sebe tvrdí opak"
        ],
        [
         "1253",
         "Industriel ensuite.\n",
         "fr",
         "2",
         "it",
         "{'fr': 0.093, 'it': 0.093, 'de': 0.09, 'es': 0.089, 'pt': 0.086, 'da': 0.086, 'sv': 0.084, 'sl': 0.082, 'cs': 0.08, 'fi': 0.08, 'pl': 0.079, 'el': 0.059}",
         "industriel ensuite"
        ],
        [
         "1432",
         "ECU 870 millioner.\n",
         "da",
         "2",
         "sv",
         "{'sv': 0.097, 'da': 0.096, 'es': 0.09, 'it': 0.089, 'de': 0.087, 'fr': 0.086, 'pt': 0.085, 'fi': 0.083, 'pl': 0.077, 'sl': 0.076, 'cs': 0.076, 'el': 0.058}",
         "ecu millioner"
        ],
        [
         "1596",
         "Realmente, isto vai demasiado longe!\"\n",
         "pt",
         "5",
         "it",
         "{'pt': 0.094, 'it': 0.094, 'es': 0.093, 'sl': 0.085, 'da': 0.084, 'sv': 0.083, 'fr': 0.083, 'de': 0.082, 'cs': 0.081, 'fi': 0.081, 'pl': 0.08, 'el': 0.059}",
         "realmente isto vai demasiado longe"
        ],
        [
         "1749",
         "Tres aspectos.\n",
         "es",
         "2",
         "pt",
         "{'pt': 0.108, 'es': 0.106, 'fr': 0.094, 'sl': 0.083, 'cs': 0.081, 'pl': 0.08, 'it': 0.08, 'sv': 0.078, 'da': 0.078, 'fi': 0.077, 'de': 0.076, 'el': 0.059}",
         "tres aspectos"
        ],
        [
         "1793",
         "Ifølge Eurostat var ca.\n",
         "da",
         "4",
         "it",
         "{'it': 0.091, 'pt': 0.09, 'sv': 0.089, 'es': 0.087, 'da': 0.087, 'fr': 0.086, 'de': 0.084, 'fi': 0.084, 'pl': 0.083, 'sl': 0.081, 'cs': 0.08, 'el': 0.059}",
         "ifølge eurostat var ca"
        ],
        [
         "1985",
         "Está encerrado o debate.\n",
         "pt",
         "4",
         "es",
         "{'es': 0.105, 'pt': 0.103, 'it': 0.087, 'fr': 0.086, 'sl': 0.083, 'cs': 0.083, 'pl': 0.081, 'de': 0.079, 'sv': 0.079, 'da': 0.079, 'fi': 0.074, 'el': 0.059}",
         "está encerrado o debate"
        ],
        [
         "1995",
         "Naturligtvis erbjuder de olympiska spelen politiska valmöjligheter.\n",
         "sv",
         "7",
         "da",
         "{'da': 0.093, 'sv': 0.092, 'sl': 0.086, 'cs': 0.085, 'fr': 0.084, 'pt': 0.084, 'fi': 0.084, 'es': 0.083, 'de': 0.083, 'it': 0.082, 'pl': 0.08, 'el': 0.065}",
         "naturligtvis erbjuder de olympiska spelen politiska valmöjligheter"
        ],
        [
         "1998",
         "Vielen Dank, Herr Kommissar.\n",
         "de",
         "4",
         "sv",
         "{'sv': 0.109, 'da': 0.099, 'de': 0.096, 'fi': 0.085, 'fr': 0.083, 'sl': 0.081, 'it': 0.081, 'cs': 0.079, 'es': 0.077, 'pt': 0.077, 'pl': 0.076, 'el': 0.058}",
         "vielen dank herr kommissar"
        ],
        [
         "2099",
         "Syriens stormufti. -\n",
         "sv",
         "2",
         "it",
         "{'it': 0.089, 'es': 0.087, 'sl': 0.086, 'cs': 0.086, 'fi': 0.086, 'da': 0.085, 'sv': 0.084, 'fr': 0.084, 'pl': 0.083, 'de': 0.083, 'pt': 0.083, 'el': 0.062}",
         "syriens stormufti"
        ],
        [
         "2112",
         "(The sitting closed at 16.35)\n",
         "sl",
         "4",
         "da",
         "{'da': 0.094, 'sv': 0.09, 'de': 0.088, 'cs': 0.087, 'fi': 0.086, 'fr': 0.084, 'it': 0.084, 'sl': 0.083, 'es': 0.082, 'pl': 0.08, 'pt': 0.08, 'el': 0.061}",
         "the sitting closed at"
        ],
        [
         "2133",
         "I vilket syfte?\n",
         "sv",
         "3",
         "da",
         "{'da': 0.098, 'sv': 0.097, 'fr': 0.087, 'it': 0.086, 'sl': 0.085, 'de': 0.085, 'fi': 0.083, 'cs': 0.082, 'pl': 0.079, 'es': 0.079, 'pt': 0.078, 'el': 0.061}",
         "i vilket syfte"
        ],
        [
         "2344",
         "Vil der komme et svar fra Parlamentet?\n",
         "da",
         "7",
         "sv",
         "{'sv': 0.111, 'da': 0.105, 'de': 0.086, 'fr': 0.084, 'it': 0.083, 'es': 0.082, 'pt': 0.081, 'sl': 0.08, 'fi': 0.08, 'cs': 0.079, 'pl': 0.073, 'el': 0.056}",
         "vil der komme et svar fra parlamentet"
        ],
        [
         "2375",
         "Citat slut.\n",
         "da",
         "2",
         "fr",
         "{'fr': 0.095, 'sv': 0.091, 'cs': 0.09, 'it': 0.087, 'da': 0.087, 'de': 0.084, 'pt': 0.082, 'fi': 0.081, 'pl': 0.08, 'es': 0.08, 'sl': 0.08, 'el': 0.062}",
         "citat slut"
        ],
        [
         "2571",
         "Está encerrado o debate.\n",
         "pt",
         "4",
         "es",
         "{'es': 0.105, 'pt': 0.103, 'it': 0.087, 'fr': 0.086, 'sl': 0.083, 'cs': 0.083, 'pl': 0.081, 'de': 0.079, 'sv': 0.079, 'da': 0.079, 'fi': 0.074, 'el': 0.059}",
         "está encerrado o debate"
        ],
        [
         "2578",
         "(Posiedzenie zostało otwarte o godz.\n",
         "pt",
         "5",
         "pl",
         "{'pl': 0.105, 'sl': 0.09, 'cs': 0.087, 'pt': 0.086, 'es': 0.084, 'it': 0.084, 'de': 0.082, 'fr': 0.08, 'sv': 0.079, 'da': 0.079, 'fi': 0.079, 'el': 0.066}",
         "posiedzenie zostało otwarte o godz"
        ],
        [
         "2623",
         "De novo contra Schuman.\n",
         "pt",
         "4",
         "es",
         "{'es': 0.094, 'pt': 0.094, 'it': 0.093, 'fr': 0.09, 'sv': 0.087, 'de': 0.086, 'da': 0.084, 'cs': 0.081, 'sl': 0.08, 'pl': 0.077, 'fi': 0.077, 'el': 0.057}",
         "de novo contra schuman"
        ],
        [
         "2754",
         "Kommissionen har gjort sin pligt.\n",
         "da",
         "5",
         "sv",
         "{'sv': 0.113, 'da': 0.107, 'de': 0.089, 'fi': 0.088, 'fr': 0.084, 'it': 0.083, 'es': 0.078, 'sl': 0.077, 'cs': 0.075, 'pt': 0.075, 'pl': 0.074, 'el': 0.058}",
         "kommissionen har gjort sin pligt"
        ],
        [
         "2765",
         "Det tar verkligen priset.\n",
         "sv",
         "4",
         "da",
         "{'da': 0.112, 'sv': 0.109, 'de': 0.093, 'sl': 0.082, 'fr': 0.082, 'it': 0.08, 'cs': 0.079, 'fi': 0.079, 'es': 0.078, 'pt': 0.078, 'pl': 0.073, 'el': 0.056}",
         "det tar verkligen priset"
        ],
        [
         "2793",
         "I USA betaler man blot 1 850 EUR.\n",
         "da",
         "6",
         "sv",
         "{'sv': 0.095, 'da': 0.095, 'fr': 0.089, 'de': 0.087, 'it': 0.086, 'fi': 0.083, 'pl': 0.082, 'es': 0.082, 'pt': 0.081, 'sl': 0.08, 'cs': 0.078, 'el': 0.06}",
         "i usa betaler man blot eur"
        ],
        [
         "2795",
         "Formulerei dunque alcune domande.\n",
         "it",
         "4",
         "da",
         "{'da': 0.093, 'fr': 0.091, 'pt': 0.091, 'it': 0.091, 'es': 0.089, 'sv': 0.085, 'de': 0.084, 'pl': 0.079, 'sl': 0.079, 'cs': 0.079, 'fi': 0.076, 'el': 0.063}",
         "formulerei dunque alcune domande"
        ],
        [
         "2797",
         "Men vi behöver även en skicklig bemanning.\n",
         "sv",
         "7",
         "da",
         "{'sv': 0.1, 'da': 0.1, 'de': 0.09, 'fi': 0.085, 'sl': 0.082, 'fr': 0.082, 'es': 0.081, 'it': 0.081, 'cs': 0.08, 'pt': 0.079, 'pl': 0.078, 'el': 0.064}",
         "men vi behöver även en skicklig bemanning"
        ],
        [
         "3468",
         "Está encerrado o debate.\n",
         "pt",
         "4",
         "es",
         "{'es': 0.105, 'pt': 0.103, 'it': 0.087, 'fr': 0.086, 'sl': 0.083, 'cs': 0.083, 'pl': 0.081, 'de': 0.079, 'sv': 0.079, 'da': 0.079, 'fi': 0.074, 'el': 0.059}",
         "está encerrado o debate"
        ],
        [
         "3561",
         "¡Qué caradura!\n",
         "es",
         "2",
         "pt",
         "{'pt': 0.099, 'es': 0.097, 'it': 0.094, 'fr': 0.089, 'pl': 0.083, 'cs': 0.083, 'sv': 0.082, 'sl': 0.08, 'fi': 0.078, 'de': 0.075, 'da': 0.075, 'el': 0.066}",
         "qué caradura"
        ],
        [
         "3723",
         "Zaprosił nas Pan na obiad.\n",
         "pl",
         "5",
         "sl",
         "{'pl': 0.093, 'sl': 0.093, 'cs': 0.09, 'es': 0.088, 'pt': 0.086, 'it': 0.083, 'sv': 0.082, 'fr': 0.082, 'fi': 0.081, 'de': 0.079, 'da': 0.079, 'el': 0.064}",
         "zaprosił nas pan na obiad"
        ],
        [
         "3784",
         "Zaproponowano mi objęcie opieki nad panem Dmitrijem Bondarenko.\n",
         "pl",
         "8",
         "sl",
         "{'pl': 0.094, 'sl': 0.094, 'cs': 0.088, 'it': 0.085, 'pt': 0.084, 'es': 0.083, 'de': 0.082, 'sv': 0.081, 'fr': 0.081, 'da': 0.081, 'fi': 0.08, 'el': 0.067}",
         "zaproponowano mi objęcie opieki nad panem dmitrijem bondarenko"
        ],
        [
         "3822",
         "Hur behandlas de?\n",
         "sv",
         "3",
         "da",
         "{'da': 0.095, 'es': 0.094, 'sv': 0.092, 'de': 0.09, 'fr': 0.09, 'pt': 0.089, 'it': 0.081, 'sl': 0.08, 'cs': 0.079, 'pl': 0.077, 'fi': 0.074, 'el': 0.059}",
         "hur behandlas de"
        ],
        [
         "3839",
         "Program Marco Polo II (\n",
         "cs",
         "4",
         "sl",
         "{'sl': 0.097, 'it': 0.093, 'pl': 0.092, 'cs': 0.091, 'pt': 0.09, 'es': 0.087, 'fr': 0.081, 'sv': 0.078, 'da': 0.078, 'de': 0.076, 'fi': 0.076, 'el': 0.061}",
         "program marco polo ii"
        ],
        [
         "3935",
         "Está encerrado o debate.\n",
         "pt",
         "4",
         "es",
         "{'es': 0.105, 'pt': 0.103, 'it': 0.087, 'fr': 0.086, 'sl': 0.083, 'cs': 0.083, 'pl': 0.081, 'de': 0.079, 'sv': 0.079, 'da': 0.079, 'fi': 0.074, 'el': 0.059}",
         "está encerrado o debate"
        ],
        [
         "4070",
         "Osiem procent!\n",
         "pl",
         "2",
         "fr",
         "{'fr': 0.094, 'pl': 0.092, 'cs': 0.091, 'pt': 0.09, 'de': 0.087, 'sv': 0.084, 'sl': 0.083, 'da': 0.083, 'es': 0.082, 'it': 0.082, 'fi': 0.076, 'el': 0.056}",
         "osiem procent"
        ],
        [
         "4116",
         "Vůbec ne.\n",
         "cs",
         "2",
         "sl",
         "{'sl': 0.094, 'cs': 0.093, 'it': 0.09, 'fr': 0.088, 'da': 0.088, 'sv': 0.084, 'pt': 0.083, 'es': 0.082, 'de': 0.082, 'pl': 0.081, 'fi': 0.072, 'el': 0.063}",
         "vůbec ne"
        ],
        [
         "4233",
         "Permítanme que les explique tres de ellas.\n",
         "es",
         "7",
         "fr",
         "{'es': 0.107, 'fr': 0.107, 'pt': 0.097, 'it': 0.087, 'da': 0.079, 'sl': 0.078, 'cs': 0.078, 'sv': 0.078, 'de': 0.077, 'fi': 0.076, 'pl': 0.075, 'el': 0.062}",
         "permítanme que les explique tres de ellas"
        ],
        [
         "4464",
         "Stálo to za to?\n",
         "cs",
         "4",
         "sl",
         "{'sl': 0.104, 'cs': 0.099, 'it': 0.092, 'es': 0.091, 'pt': 0.09, 'pl': 0.089, 'fi': 0.078, 'de': 0.074, 'sv': 0.074, 'fr': 0.074, 'da': 0.073, 'el': 0.062}",
         "stálo to za to"
        ],
        [
         "4482",
         "\"because [he has] nothing else here for you,\n",
         "sl",
         "8",
         "da",
         "{'da': 0.097, 'de': 0.09, 'es': 0.088, 'sv': 0.087, 'fr': 0.085, 'pt': 0.085, 'it': 0.084, 'cs': 0.082, 'pl': 0.079, 'sl': 0.079, 'fi': 0.079, 'el': 0.065}",
         "because he has nothing else here for you"
        ],
        [
         "4766",
         "Pactio Olisipio censenda est.\n",
         "sl",
         "4",
         "fr",
         "{'fr': 0.092, 'pt': 0.09, 'es': 0.089, 'it': 0.087, 'cs': 0.084, 'pl': 0.083, 'sl': 0.083, 'de': 0.083, 'fi': 0.083, 'da': 0.082, 'sv': 0.081, 'el': 0.061}",
         "pactio olisipio censenda est"
        ],
        [
         "4867",
         "Ahora es preciso volver a reactivar la maquinaria.\n",
         "es",
         "8",
         "pt",
         "{'pt': 0.099, 'es': 0.097, 'it': 0.092, 'fr': 0.087, 'sl': 0.083, 'sv': 0.082, 'de': 0.081, 'cs': 0.08, 'da': 0.08, 'fi': 0.08, 'pl': 0.077, 'el': 0.062}",
         "ahora es preciso volver a reactivar la maquinaria"
        ],
        [
         "4926",
         "Jak je na tom Nizozemsko?\n",
         "cs",
         "5",
         "sl",
         "{'sl': 0.099, 'cs': 0.099, 'pl': 0.098, 'sv': 0.084, 'fi': 0.082, 'pt': 0.081, 'da': 0.081, 'fr': 0.079, 'es': 0.078, 'it': 0.078, 'de': 0.076, 'el': 0.063}",
         "jak je na tom nizozemsko"
        ],
        [
         "4976",
         "- Bericht: McCarthy\n",
         "pl",
         "2",
         "de",
         "{'de': 0.1, 'pl': 0.086, 'cs': 0.086, 'sv': 0.085, 'es': 0.084, 'it': 0.084, 'da': 0.084, 'fr': 0.083, 'pt': 0.081, 'sl': 0.08, 'fi': 0.079, 'el': 0.066}",
         "bericht mccarthy"
        ],
        [
         "4984",
         "Informe Gargani (A5-0428/2001)\n",
         "es",
         "2",
         "it",
         "{'it': 0.093, 'es': 0.092, 'pt': 0.087, 'sv': 0.086, 'sl': 0.085, 'da': 0.085, 'pl': 0.083, 'de': 0.083, 'fr': 0.083, 'cs': 0.08, 'fi': 0.08, 'el': 0.061}",
         "informe gargani a"
        ],
        [
         "4990",
         "   – Vielen Dank, Herr Varela.\n",
         "de",
         "4",
         "sv",
         "{'sv': 0.1, 'de': 0.092, 'da': 0.088, 'sl': 0.085, 'it': 0.085, 'fi': 0.085, 'es': 0.084, 'cs': 0.082, 'fr': 0.082, 'pt': 0.08, 'pl': 0.077, 'el': 0.061}",
         "vielen dank herr varela"
        ],
        [
         "5094",
         "Está encerrado o debate.\n",
         "pt",
         "4",
         "es",
         "{'es': 0.105, 'pt': 0.103, 'it': 0.087, 'fr': 0.086, 'sl': 0.083, 'cs': 0.083, 'pl': 0.081, 'de': 0.079, 'sv': 0.079, 'da': 0.079, 'fi': 0.074, 'el': 0.059}",
         "está encerrado o debate"
        ],
        [
         "5142",
         "Está encerrado o debate.\n",
         "pt",
         "4",
         "es",
         "{'es': 0.105, 'pt': 0.103, 'it': 0.087, 'fr': 0.086, 'sl': 0.083, 'cs': 0.083, 'pl': 0.081, 'de': 0.079, 'sv': 0.079, 'da': 0.079, 'fi': 0.074, 'el': 0.059}",
         "está encerrado o debate"
        ],
        [
         "5253",
         "Angående: Athenasystemet\n",
         "sv",
         "2",
         "da",
         "{'da': 0.098, 'sv': 0.094, 'de': 0.09, 'fr': 0.086, 'pt': 0.083, 'es': 0.082, 'it': 0.082, 'fi': 0.082, 'cs': 0.081, 'pl': 0.08, 'sl': 0.079, 'el': 0.063}",
         "angående athenasystemet"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 85
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>n_words</th>\n",
       "      <th>pred</th>\n",
       "      <th>scores</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>E' importante dirlo.\\n</td>\n",
       "      <td>it</td>\n",
       "      <td>3</td>\n",
       "      <td>es</td>\n",
       "      <td>{'es': 0.102, 'it': 0.1, 'pt': 0.099, 'fr': 0....</td>\n",
       "      <td>e' importante dirlo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>(La sesión, suspendida a las 13.10 horas, se r...</td>\n",
       "      <td>pl</td>\n",
       "      <td>11</td>\n",
       "      <td>es</td>\n",
       "      <td>{'es': 0.11, 'pt': 0.091, 'fr': 0.088, 'it': 0...</td>\n",
       "      <td>la sesión suspendida a las horas se reanuda a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>Han fyllde visst år i går.\\n</td>\n",
       "      <td>sv</td>\n",
       "      <td>6</td>\n",
       "      <td>da</td>\n",
       "      <td>{'da': 0.101, 'sv': 0.099, 'de': 0.086, 'fr': ...</td>\n",
       "      <td>han fyllde visst år i går</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>Primero vamos a votar el corrigendum nº 1, par...</td>\n",
       "      <td>es</td>\n",
       "      <td>14</td>\n",
       "      <td>pt</td>\n",
       "      <td>{'pt': 0.097, 'es': 0.096, 'it': 0.09, 'sl': 0...</td>\n",
       "      <td>primero vamos a votar el corrigendum nº para v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>- Έκθεση Costa\\n</td>\n",
       "      <td>el</td>\n",
       "      <td>2</td>\n",
       "      <td>pt</td>\n",
       "      <td>{'pt': 0.09, 'it': 0.09, 'es': 0.088, 'fi': 0....</td>\n",
       "      <td>έκθεση costa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8585</th>\n",
       "      <td>Está encerrado o debate.\\n</td>\n",
       "      <td>pt</td>\n",
       "      <td>4</td>\n",
       "      <td>es</td>\n",
       "      <td>{'es': 0.105, 'pt': 0.103, 'it': 0.087, 'fr': ...</td>\n",
       "      <td>está encerrado o debate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8660</th>\n",
       "      <td>To bezdyskusyjne.\\n</td>\n",
       "      <td>pl</td>\n",
       "      <td>2</td>\n",
       "      <td>sl</td>\n",
       "      <td>{'sl': 0.097, 'pl': 0.096, 'cs': 0.096, 'da': ...</td>\n",
       "      <td>to bezdyskusyjne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8708</th>\n",
       "      <td>Hvala lepa.\\n</td>\n",
       "      <td>sl</td>\n",
       "      <td>2</td>\n",
       "      <td>fi</td>\n",
       "      <td>{'fi': 0.091, 'es': 0.089, 'sl': 0.089, 'it': ...</td>\n",
       "      <td>hvala lepa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8759</th>\n",
       "      <td>Terrorism i Spanien\\n</td>\n",
       "      <td>sv</td>\n",
       "      <td>3</td>\n",
       "      <td>pl</td>\n",
       "      <td>{'pl': 0.089, 'de': 0.088, 'sv': 0.088, 'da': ...</td>\n",
       "      <td>terrorism i spanien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8777</th>\n",
       "      <td>Under de senaste tio åren har Afrika enligt FN...</td>\n",
       "      <td>sv</td>\n",
       "      <td>11</td>\n",
       "      <td>da</td>\n",
       "      <td>{'da': 0.098, 'sv': 0.097, 'de': 0.089, 'es': ...</td>\n",
       "      <td>under de senaste tio åren har afrika enligt fn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text language  n_words  \\\n",
       "166                              E' importante dirlo.\\n       it        3   \n",
       "394   (La sesión, suspendida a las 13.10 horas, se r...       pl       11   \n",
       "681                        Han fyllde visst år i går.\\n       sv        6   \n",
       "740   Primero vamos a votar el corrigendum nº 1, par...       es       14   \n",
       "903                                    - Έκθεση Costa\\n       el        2   \n",
       "...                                                 ...      ...      ...   \n",
       "8585                         Está encerrado o debate.\\n       pt        4   \n",
       "8660                                To bezdyskusyjne.\\n       pl        2   \n",
       "8708                                      Hvala lepa.\\n       sl        2   \n",
       "8759                              Terrorism i Spanien\\n       sv        3   \n",
       "8777  Under de senaste tio åren har Afrika enligt FN...       sv       11   \n",
       "\n",
       "     pred                                             scores  \\\n",
       "166    es  {'es': 0.102, 'it': 0.1, 'pt': 0.099, 'fr': 0....   \n",
       "394    es  {'es': 0.11, 'pt': 0.091, 'fr': 0.088, 'it': 0...   \n",
       "681    da  {'da': 0.101, 'sv': 0.099, 'de': 0.086, 'fr': ...   \n",
       "740    pt  {'pt': 0.097, 'es': 0.096, 'it': 0.09, 'sl': 0...   \n",
       "903    pt  {'pt': 0.09, 'it': 0.09, 'es': 0.088, 'fi': 0....   \n",
       "...   ...                                                ...   \n",
       "8585   es  {'es': 0.105, 'pt': 0.103, 'it': 0.087, 'fr': ...   \n",
       "8660   sl  {'sl': 0.097, 'pl': 0.096, 'cs': 0.096, 'da': ...   \n",
       "8708   fi  {'fi': 0.091, 'es': 0.089, 'sl': 0.089, 'it': ...   \n",
       "8759   pl  {'pl': 0.089, 'de': 0.088, 'sv': 0.088, 'da': ...   \n",
       "8777   da  {'da': 0.098, 'sv': 0.097, 'de': 0.089, 'es': ...   \n",
       "\n",
       "                                      preprocessed_text  \n",
       "166                                 e' importante dirlo  \n",
       "394   la sesión suspendida a las horas se reanuda a ...  \n",
       "681                           han fyllde visst år i går  \n",
       "740   primero vamos a votar el corrigendum nº para v...  \n",
       "903                                        έκθεση costa  \n",
       "...                                                 ...  \n",
       "8585                            está encerrado o debate  \n",
       "8660                                   to bezdyskusyjne  \n",
       "8708                                         hvala lepa  \n",
       "8759                                terrorism i spanien  \n",
       "8777  under de senaste tio åren har afrika enligt fn...  \n",
       "\n",
       "[85 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nos centramos en los errores\n",
    "df_errors = df_test[preds!=y_test].copy()\n",
    "\n",
    "# Añadimos la clase predicha\n",
    "df_errors.loc[:,\"pred\"] = [pred for pred, true in zip(preds, y_test) if pred != true]\n",
    "\n",
    "# Añadimos el rank\n",
    "df_errors.loc[:,\"scores\"] = [distances2scores(ngrams.predict(text,distances=True)) for i, text in enumerate(test_texts) if preds[i]!=y_test[i]]\n",
    "\n",
    "# Añadimos la frase preprocesada\n",
    "df_errors.loc[:,\"preprocessed_text\"] = [ngrams._preprocess_texts([text])[0] for i, text in enumerate(test_texts) if preds[i]!=y_test[i]]\n",
    "\n",
    "# Veamos los errores\n",
    "df_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d6edaf",
   "metadata": {},
   "source": [
    "- El dataset original contiene errores: \n",
    "    - \"(La sesión, suspendida a las 13.10 horas, se reanuda a las 15.00 horas)\" aparece como polaco.\n",
    "    - \"(The sitting closed at 16.35) parece como esloveno\n",
    "    - \"Program Marco Polo II (\" aparece como checo.\n",
    "    - \"because [he has] nothing else here for you,\" aparece como esloveno.\n",
    "- Hay frases muy complicadas o casi imposibles:\n",
    "    - \"(10419/1/2001 - C5-0417/2001 - 2000/0188(COD)) (Εισηγήτρια: κ. Niebler)\" es griega, pero solo tiene una palabra en griego\n",
    "    - \"ECU 870 millioner.\" es danés.\n",
    "- Casos en los que por la forma en la que trabaja el modelo, es comprensible el error:\n",
    "    - \"Está encerrado o debate.\" es portuguesa y se clasifica como español. La única diferencia es el \"o\". Es un caso complicado, habría que usar algún mecanismo de atención.\n",
    "- Frases comunes en distintas lenguas:\n",
    "    - \"Al contrario!\" aparece como italiano, pero podría ser español. Diferencia en el uso de los signos de puntuació en los distintos idiomas.\n",
    "- Cuando el modelo se equivoca, suele haber muy poca diferencia en los scores entre el idioma correcto y el idioma predicho\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef639081",
   "metadata": {},
   "source": [
    "### Pruebas\n",
    "Hacemos algunas pruebas para ver el funcionamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6debdcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'es'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams.predict(\"Hola buenos días\") # Español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6729e547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams.predict(\"Cómo\") # Español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7fe0ee83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams.predict(\"Nicht\") # Alemán "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49654102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fr'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams.predict(\"Ça ne marche pas\") # Francés "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f5e6692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fr'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams.predict(\"Ça ne marche pas, je suis desolé\") # Francés "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a0a37f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sv'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams.predict(\"Det fungerar inte.\") # Sueco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c36e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc03252",
   "metadata": {},
   "source": [
    "## 3. Aumentamos el tamaño del perfil de ngramas por idioma\n",
    "Si aumentamos el tamaño del perfil de n-gramas de 1000 a 5000 el rendimiento del modelo se dispara, pero aumentan también su latencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el modelo\n",
    "ngramsXL = NGramLanguageIdentifier(n=5, top_k=5000)\n",
    "\n",
    "# Lo entrenamos\n",
    "ngramsXL.fit(train_texts, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7087ec1e",
   "metadata": {},
   "source": [
    "Vamos a hacer las predicciones sobre el conjunto test miendo el tiempo que tarda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1eed5e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latencia: 3738.64 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# Hacemos las predicciones\n",
    "preds = ngramsXL.predict(test_texts)\n",
    "end = time.time()\n",
    "\n",
    "latency_ms = (end - start) * 1000  # milisegundos\n",
    "print(f\"Latencia: {latency_ms:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce4494fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Global Metrics ===\n",
      " accuracy  precision_macro  recall_macro  f1_macro\n",
      " 0.997111          0.99711      0.997094    0.9971\n",
      "\n",
      "=== Per-Class Metrics ===\n",
      "label  precision   recall       f1  support\n",
      "   cs   1.000000 0.995845 0.997918      722\n",
      "   da   0.993307 0.994638 0.993972      746\n",
      "   de   0.997379 0.998688 0.998033      762\n",
      "   el   1.000000 1.000000 1.000000      733\n",
      "   es   0.993132 0.994498 0.993814      727\n",
      "   fi   1.000000 1.000000 1.000000      785\n",
      "   fr   0.994758 1.000000 0.997372      759\n",
      "   it   0.995962 0.998650 0.997305      741\n",
      "   pl   0.997283 0.997283 0.997283      736\n",
      "   pt   0.996016 0.994695 0.995355      754\n",
      "   sl   1.000000 0.994602 0.997294      741\n",
      "   sv   0.997481 0.996226 0.996853      795\n",
      "\n",
      "=== Metrics Per Size ===\n",
      "  size  accuracy  precision_macro  recall_macro  f1_macro\n",
      " large  1.000000         1.000000      1.000000  1.000000\n",
      "medium  0.999332         0.999325      0.999325  0.999324\n",
      " small  0.987050         0.986765      0.986885  0.986788\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "     cs   da   de   el   es   fi   fr   it   pl   pt   sl   sv\n",
      "cs  719    0    0    0    0    0    1    1    1    0    0    0\n",
      "da    0  742    1    0    0    0    1    0    0    0    0    2\n",
      "de    0    0  761    0    0    0    1    0    0    0    0    0\n",
      "el    0    0    0  733    0    0    0    0    0    0    0    0\n",
      "es    0    0    0    0  723    0    0    1    0    3    0    0\n",
      "fi    0    0    0    0    0  785    0    0    0    0    0    0\n",
      "fr    0    0    0    0    0    0  759    0    0    0    0    0\n",
      "it    0    0    0    0    1    0    0  740    0    0    0    0\n",
      "pl    0    0    1    0    1    0    0    0  734    0    0    0\n",
      "pt    0    0    0    0    3    0    0    0    1  750    0    0\n",
      "sl    0    2    0    0    0    0    1    1    0    0  737    0\n",
      "sv    0    3    0    0    0    0    0    0    0    0    0  792\n"
     ]
    }
   ],
   "source": [
    "# Empleamos una función personalizada para ver un report del rendimiento\n",
    "results_ngramXL = classification_metrics_report(y_test, preds, df_test[\"n_words\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f451cf7",
   "metadata": {},
   "source": [
    "Medimos la latencia del modelo que proponen en el paper con tamaño 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82eb3b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el modelo\n",
    "ngramsS = NGramLanguageIdentifier(n=5, top_k=600)\n",
    "\n",
    "# Lo entrenamos\n",
    "ngramsS.fit(train_texts, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ccc88b",
   "metadata": {},
   "source": [
    "Vamos a hacer las predicciones sobre el conjunto test miendo el tiempo que tarda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bab04268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latencia: 2743.30 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# Hacemos las predicciones\n",
    "preds = ngramsS.predict(test_texts)\n",
    "end = time.time()\n",
    "\n",
    "latency_ms = (end - start) * 1000  # milisegundos\n",
    "print(f\"Latencia: {latency_ms:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "010a4063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Global Metrics ===\n",
      " accuracy  precision_macro  recall_macro  f1_macro\n",
      " 0.985335         0.985423      0.985289  0.985337\n",
      "\n",
      "=== Per-Class Metrics ===\n",
      "label  precision   recall       f1  support\n",
      "   cs   1.000000 0.983380 0.991620      722\n",
      "   da   0.975839 0.974531 0.975184      746\n",
      "   de   0.994709 0.986877 0.990777      762\n",
      "   el   1.000000 0.993179 0.996578      733\n",
      "   es   0.963165 0.971114 0.967123      727\n",
      "   fi   0.997459 1.000000 0.998728      785\n",
      "   fr   0.989474 0.990777 0.990125      759\n",
      "   it   0.986577 0.991903 0.989233      741\n",
      "   pl   0.994536 0.989130 0.991826      736\n",
      "   pt   0.973298 0.966844 0.970060      754\n",
      "   sl   0.977454 0.994602 0.985953      741\n",
      "   sv   0.972569 0.981132 0.976832      795\n",
      "\n",
      "=== Metrics Per Size ===\n",
      "  size  accuracy  precision_macro  recall_macro  f1_macro\n",
      " large  0.998538         0.998578      0.998578  0.998578\n",
      "medium  0.993985         0.994107      0.993984  0.994041\n",
      " small  0.943131         0.942306      0.942352  0.941954\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "     cs   da   de   el   es   fi   fr   it   pl   pt   sl   sv\n",
      "cs  710    0    0    0    0    0    0    1    0    0   11    0\n",
      "da    0  727    1    0    0    0    2    1    0    0    0   15\n",
      "de    0    1  752    0    1    0    3    0    0    0    0    5\n",
      "el    0    1    0  728    0    0    0    0    2    1    1    0\n",
      "es    0    2    0    0  706    0    1    1    0   17    0    0\n",
      "fi    0    0    0    0    0  785    0    0    0    0    0    0\n",
      "fr    0    0    1    0    2    0  752    2    0    0    0    2\n",
      "it    0    1    0    0    3    0    0  735    0    2    0    0\n",
      "pl    0    0    1    0    1    0    1    0  728    0    5    0\n",
      "pt    0    0    1    0   19    0    0    4    1  729    0    0\n",
      "sl    0    2    0    0    0    1    1    0    0    0  737    0\n",
      "sv    0   11    0    0    1    1    0    1    1    0    0  780\n"
     ]
    }
   ],
   "source": [
    "# Empleamos una función personalizada para ver un report del rendimiento\n",
    "results_ngramS = classification_metrics_report(y_test, preds, df_test[\"n_words\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
