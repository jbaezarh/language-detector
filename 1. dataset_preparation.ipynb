{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ca89fbf",
   "metadata": {},
   "source": [
    "# Preparación del dataset\n",
    "\n",
    "En este notebook crearemos el dataset que será nuestro corpus para entrenar, ajustar y evaluar los modelos de detección automática de idiomas que se desarrollarán posteriormente. Nos ajustaremos a las directrices indicadas:\n",
    " \n",
    "> - Seleccionar al menos 7 idiomas diferentes del corpus  \n",
    "> - Extraer entre 1.000 y 10.000 frases por idioma, garantizando un balanceo entre las clases  \n",
    "> - Controlar la longitud de las frases:  \n",
    ">     - Mínimo: 2-3 palabras  \n",
    ">     - Máximo: 15-20 palabras  \n",
    ">     - Media objetivo: 6-7 palabras  \n",
    "> - Dividir adecuadamente los datos en conjuntos de entrenamiento (70%), validación (15%) y prueba (15%)  \n",
    "> - Documentar todo el proceso de preparación de datos  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d27c2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librererías necesarias\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "import re\n",
    "import random \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f776786",
   "metadata": {},
   "source": [
    "Definiremos una función que nos permitirá descargar y procesar las urls proporcionadas para crear nuestro dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71b71552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta primera función será la que empleemos para tomar frases aleatorias de los archivos descargados, de acuerdo a nuestras restricciones de longitud\n",
    "\n",
    "def read_file(path, n_sentences, random_seed=None, min_len = 3, max_len=1):\n",
    "    \"\"\"\n",
    "    Lee un archivo de texto y devuelve un DataFrame con una muestra aleatoria de frases \n",
    "    que cumplen con una longitud específica de palabras.\n",
    "\n",
    "    Args:\n",
    "    - path (str): Ruta del archivo de texto a leer.\n",
    "    - n_sentences (int): Número de frases que se desean extraer.\n",
    "    - random_seed (int): Semilla aleatoria para hacerlo reproducible\n",
    "    - min_len (int, opcional): Número mínimo de palabras por frase (por defecto, 3).\n",
    "    - max_len (int, opcional): Número máximo de palabras por frase (por defecto, 15).\n",
    "\n",
    "    Values:\n",
    "    - pd.DataFrame: Contiene las frases seleccionadas, el idioma y el número de palabras.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path, \"r\", encoding = \"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Filtramos las que cumplen nuestra condición de tamaño\n",
    "    filtered_lines = [\n",
    "    (s, len(words)) for s in lines\n",
    "    if (words := re.findall(r'\\b[^\\W\\d_]+\\b', s, flags=re.UNICODE)) and min_len <= len(words) <= max_len\n",
    "    ]   \n",
    "    # Esta expresión regular retiene las palabras (secuencias de caracteres UNICODE\n",
    "    # que no se que no sean dígitos, signos de putuación, espacios o \"_\", i.e. que sean \n",
    "    # letras, limitados por \"word boundaries\" como espacios o signos de puntuación)\n",
    "    # Esta es solo una primera aproximación para hacer un primer filtrado\n",
    "\n",
    "    # Tomamos una muestra aleatoria de tamaño n_rows\n",
    "    if random_seed:\n",
    "        random.seed(random_seed) \n",
    "    sample = random.sample(filtered_lines,n_sentences)\n",
    "\n",
    "    # Separamos las frases y la longitud\n",
    "    sentences, n_words = zip(*sample)\n",
    "\n",
    "    # Creamos el dataframe\n",
    "    _,ext = os.path.splitext(path)\n",
    "    df = pd.DataFrame({\"text\": sentences,\n",
    "                       \"language\": [ext.replace(\".\",\"\") for _ in range(n_sentences)],\n",
    "                       \"n_words\": n_words})\n",
    "    return df\n",
    "\n",
    "# Esta será la función que efectivamente cree el dataset\n",
    "\n",
    "def get_dataset(urls, dir=None, download_folder=None, download_files=True, del_extra = True, n_sentences=5000, min_len=2, max_len=15, random_seed=None):\n",
    "    \"\"\"\n",
    "    Descarga, descomprime y procesa archivos de texto multilingües desde una lista de URLs para construir un dataset.\n",
    "\n",
    "    Args:\n",
    "    - urls (list): Lista de URLs de los archivos a descargar.\n",
    "    - dir (str): Carpeta base donde se guardará el dataset (por defecto: \"multilingual_europarl_dataset\").\n",
    "    - download_folder (str): Subcarpeta donde se almacenan los archivos descargados (por defecto: \"europarl_datasets\").\n",
    "    - download_files (bool): Si True, descarga los archivos desde las URLs proporcionadas.\n",
    "    - random_seed (int): Semilla aleatoria para hacerlo reproducible\n",
    "    - del_extra (bool): Si True, elimina los archivos comprimidos después de descomprimirlos y los archivos en inglés.\n",
    "    - n_sentences (int): Número máximo de oraciones a leer por archivo.\n",
    "    - min_len (int): Longitud mínima de oración permitida.\n",
    "    - max_len (int): Longitud máxima de oración permitida.\n",
    "\n",
    "    Values:\n",
    "    - DataFrame de pandas con dos columnas: 'text' (oraciones) y 'language' (idioma).\n",
    "    \"\"\"\n",
    "\n",
    "    if not dir:\n",
    "        dir = \"multilingual_europarl_dataset\"\n",
    "        if not os.path.exists(dir):\n",
    "            os.makedirs(dir, exist_ok=True)\n",
    "    \n",
    "    if not download_folder:\n",
    "        download_folder = \"europarl_datasets\"\n",
    "    \n",
    "    download_path = os.path.join(dir,download_folder)\n",
    "\n",
    "    if download_files:\n",
    "       if not os.path.exists(download_path):\n",
    "           os.makedirs(download_path, exist_ok=True)\n",
    "\n",
    "       # Procesar cada URL\n",
    "       for url in urls:\n",
    "           file_name = os.path.basename(url)\n",
    "           file_path = os.path.join(download_path, file_name)\n",
    "\n",
    "           # Descargar el archivo\n",
    "           print(f\"Descargando: {url}\")\n",
    "           resp = requests.get(url)\n",
    "           with open(file_path, 'wb') as f:\n",
    "               f.write(resp.content)\n",
    "\n",
    "           # Descomprimir si es .tgz o .tar.gz\n",
    "           if tarfile.is_tarfile(file_path):\n",
    "                print(f\"Descomprimiendo: {file_name}\")\n",
    "                with tarfile.open(file_path, 'r:gz') as tar:\n",
    "                       tar.extractall(path=download_path, \n",
    "                                      filter=lambda tarinfo, path: tarinfo if not tarinfo.name.endswith(\".en\") else None)\n",
    "\n",
    "           # Borramos archivos innecesarios\n",
    "           if del_extra:\n",
    "                os.remove(file_path)\n",
    "\n",
    "\n",
    "    # Creamos el dataset a partir de los archivos descargados\n",
    "    print(\"Creando el dataframe...\")\n",
    "\n",
    "    df_all = pd.DataFrame(columns=[\"text\", \"language\", \"n_words\"])\n",
    "    for file in os.listdir(download_path):\n",
    "        extension = os.path.splitext(file)[1]\n",
    "        if extension not in [\".tgz\",\".en\"]:\n",
    "             print(\"  Procesando: \", file)\n",
    "             df_lan = read_file(os.path.join(download_path, file), \n",
    "                                n_sentences=n_sentences,\n",
    "                                min_len=min_len,\n",
    "                                max_len=max_len,\n",
    "                                random_seed=random_seed)\n",
    "             \n",
    "        df_all = pd.concat([df_all, df_lan], ignore_index=True)\n",
    "\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397a1c89",
   "metadata": {},
   "source": [
    "Empleamos la función para crear nuestro dataset.\n",
    "- Idiomas: \n",
    "    - Alemán\n",
    "    - Español\n",
    "    - Italiano\n",
    "    - Francés\n",
    "    - Esloveno\n",
    "    - Portugués\n",
    "    - Sueco\n",
    "    - Checo\n",
    "    - Danés\n",
    "    - Polaco\n",
    "    - Griego\n",
    "    - Finés\n",
    "- Número de frases por idioma: 5000\n",
    "- Número mínimo de palabras por frase: 2\n",
    "- Número máximo de palabras por frase: 15\n",
    "\n",
    "Observación: Estamos empleando FRASES COMPLETAS, podría ser útil emplear también frases incompletas ya que puede ser una caso común de uso del detector de idioma (por ejemplo, si el usuario está escribiendo una frase para traducir puede ser útil poder detectar el idioma antes de que concluya). Estudiaremos si es necesario incluir en nuestro dataset frases incompletas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04a007c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando: https://www.statmt.org/europarl/v7/de-en.tgz\n",
      "Descomprimiendo: de-en.tgz\n",
      "Descargando: https://www.statmt.org/europarl/v7/es-en.tgz\n",
      "Descomprimiendo: es-en.tgz\n",
      "Descargando: https://www.statmt.org/europarl/v7/it-en.tgz\n",
      "Descomprimiendo: it-en.tgz\n",
      "Descargando: https://www.statmt.org/europarl/v7/fr-en.tgz\n",
      "Descomprimiendo: fr-en.tgz\n",
      "Descargando: https://www.statmt.org/europarl/v7/sl-en.tgz\n",
      "Descomprimiendo: sl-en.tgz\n",
      "Descargando: https://www.statmt.org/europarl/v7/pt-en.tgz\n",
      "Descomprimiendo: pt-en.tgz\n",
      "Descargando: https://www.statmt.org/europarl/v7/sv-en.tgz\n",
      "Descomprimiendo: sv-en.tgz\n",
      "Descargando: https://www.statmt.org/europarl/v7/cs-en.tgz\n",
      "Descomprimiendo: cs-en.tgz\n",
      "Descargando: https://www.statmt.org/europarl/v7/da-en.tgz\n",
      "Descomprimiendo: da-en.tgz\n",
      "Descargando: https://www.statmt.org/europarl/v7/pl-en.tgz\n",
      "Descomprimiendo: pl-en.tgz\n",
      "Descargando: https://www.statmt.org/europarl/v7/el-en.tgz\n",
      "Descomprimiendo: el-en.tgz\n",
      "Descargando: https://www.statmt.org/europarl/v7/fi-en.tgz\n",
      "Descomprimiendo: fi-en.tgz\n",
      "Creando el dataframe...\n",
      "  Procesando:  europarl-v7.cs-en.cs\n",
      "  Procesando:  europarl-v7.da-en.da\n",
      "  Procesando:  europarl-v7.de-en.de\n",
      "  Procesando:  europarl-v7.el-en.el\n",
      "  Procesando:  europarl-v7.es-en.es\n",
      "  Procesando:  europarl-v7.fi-en.fi\n",
      "  Procesando:  europarl-v7.fr-en.fr\n",
      "  Procesando:  europarl-v7.it-en.it\n",
      "  Procesando:  europarl-v7.pl-en.pl\n",
      "  Procesando:  europarl-v7.pt-en.pt\n",
      "  Procesando:  europarl-v7.sl-en.sl\n",
      "  Procesando:  europarl-v7.sv-en.sv\n"
     ]
    }
   ],
   "source": [
    "# urls = [\"https://www.statmt.org/europarl/v7/de-en.tgz\", # Alemán\n",
    "#         \"https://www.statmt.org/europarl/v7/es-en.tgz\", # Español\n",
    "#         \"https://www.statmt.org/europarl/v7/it-en.tgz\", # Italiano\n",
    "#         \"https://www.statmt.org/europarl/v7/fr-en.tgz\", # Francés\n",
    "#         \"https://www.statmt.org/europarl/v7/sl-en.tgz\", # Esloveno\n",
    "#         \"https://www.statmt.org/europarl/v7/pt-en.tgz\", # Portugués\n",
    "#         \"https://www.statmt.org/europarl/v7/sv-en.tgz\"] # Sueco\n",
    "\n",
    "urls = [\"https://www.statmt.org/europarl/v7/de-en.tgz\", # Alemán\n",
    "        \"https://www.statmt.org/europarl/v7/es-en.tgz\", # Español\n",
    "        \"https://www.statmt.org/europarl/v7/it-en.tgz\", # Italiano\n",
    "        \"https://www.statmt.org/europarl/v7/fr-en.tgz\", # Francés\n",
    "        \"https://www.statmt.org/europarl/v7/sl-en.tgz\", # Esloveno\n",
    "        \"https://www.statmt.org/europarl/v7/pt-en.tgz\", # Portugués\n",
    "        \"https://www.statmt.org/europarl/v7/sv-en.tgz\", # Sueco\n",
    "        \"https://www.statmt.org/europarl/v7/cs-en.tgz\", # Checo\n",
    "        \"https://www.statmt.org/europarl/v7/da-en.tgz\", # Danés\n",
    "        \"https://www.statmt.org/europarl/v7/pl-en.tgz\", # Polaco\n",
    "        \"https://www.statmt.org/europarl/v7/el-en.tgz\", # Griego\n",
    "        \"https://www.statmt.org/europarl/v7/fi-en.tgz\"] # Finés \n",
    "\n",
    "data = get_dataset(urls, dir = os.path.join(\"data\",\"raw\"), \n",
    "                   download_files=True,\n",
    "                   n_sentences=5000)\n",
    "data.to_csv(os.path.join(\"data\",\"raw\",\"multilingual_europarl_dataset.csv\"),\n",
    "            index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a03870",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74f77bf",
   "metadata": {},
   "source": [
    "## Partición en entrenamiento/validación/test\n",
    "Vamos a dividir el conjunto de datos en entrenamiento, validación y test. Para ello, definamos la siguiente función `split_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5847db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataset(df, stratify_col=None, train_size=0.7, val_size=0.15, test_size=0.15, random_state=None):\n",
    "    \"\"\"\n",
    "    Divide un DataFrame en conjuntos de entrenamiento, validación y test. Si val_size es nulo, \n",
    "    en entrenamiento y test.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): El DataFrame a dividir.\n",
    "    - stratify_col (str, opcional): Columna para estratificación.\n",
    "    - train_size (float): Proporción del conjunto de entrenamiento (por defecto, 0.7).\n",
    "    - val_size (float): Proporción del conjunto de validación (por defecto, 0.15).\n",
    "    - test_size (float): Proporción del conjunto de prueba (por defecto, 0.15).\n",
    "    - random_state (int): Semilla para reproducibilidad.\n",
    "\n",
    "    Values:\n",
    "    - train_df, val_df, test_df (pd.DataFrame): Conjuntos de entrenamiento, validación y prueba (si val_size > 0).\n",
    "    - train_df, test_df (pd.Dataframe): Conjuntos de entrenamiento y test (si val_size = 0)\n",
    "    \"\"\"\n",
    "    assert abs(train_size + val_size + test_size - 1.0) < 1e-5, \"Las proporciones deben sumar 1.\"\n",
    "\n",
    "    stratify_data = df[stratify_col] if stratify_col else None\n",
    "\n",
    "    if val_size > 0:\n",
    "        # Primera división: entrenamiento y temporal (val + test)\n",
    "        train_df, temp_df = train_test_split(\n",
    "            df, test_size=(1 - train_size), random_state=random_state, stratify=stratify_data\n",
    "        )\n",
    "\n",
    "        # Actualizamos la columna de estratificación\n",
    "        stratify_temp = temp_df[stratify_col] if stratify_col else None\n",
    "\n",
    "        # División de temporal en validación y prueba\n",
    "        relative_test_size = test_size / (val_size + test_size)  # proporcional dentro del 30%\n",
    "        val_df, test_df = train_test_split(\n",
    "            temp_df, test_size=relative_test_size, random_state=random_state, stratify=stratify_temp\n",
    "        )\n",
    "        return train_df, val_df, test_df\n",
    "    \n",
    "    else:\n",
    "        # Si val_size=0: train / test\n",
    "        train_df, test_df = train_test_split(\n",
    "            df, test_size=test_size, random_state=random_state, stratify=stratify_data\n",
    "        )\n",
    "\n",
    "        return train_df, test_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d2d8fa",
   "metadata": {},
   "source": [
    "Empleamos la función para crear nuestra partición de los datos y los guardamos para emplear siempre la misma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6370812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = split_dataset(data, random_state=123)\n",
    "df_train.to_csv(os.path.join(\"data\", \"raw\",\"train.csv\"), index=False)\n",
    "df_val.to_csv(os.path.join(\"data\", \"raw\",\"val.csv\"), index=False)\n",
    "df_test.to_csv(os.path.join(\"data\", \"raw\",\"test.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
